{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ddfa063-3c97-4a17-9fac-42d102e58274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54afbce4e64e4496b01b23431ece915b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     22\u001b[0m conversation \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     23\u001b[0m     {\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     },\n\u001b[1;32m     30\u001b[0m ]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Run VLM\u001b[39;00m\n\u001b[1;32m     33\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor_vlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m---> 35\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(model_vlm\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model_vlm\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     38\u001b[0m decoded \u001b[38;5;241m=\u001b[39m processor_vlm\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText, AutoModelForCausalLM, AutoTokenizer\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Load Vision Model (VLM)\n",
    "# ----------------------------\n",
    "model_id_vlm = \"LiquidAI/LFM2-VL-450M\"\n",
    "model_vlm = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id_vlm,\n",
    "    device_map={\"\": \"cpu\"},\n",
    "    torch_dtype=\"float32\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "processor_vlm = AutoProcessor.from_pretrained(model_id_vlm, trust_remote_code=True)\n",
    "\n",
    "# Load test image\n",
    "image_path = \"data/test_mm_7.36.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": \"Read the gauge and output only '<value> mm'.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run VLM\n",
    "inputs = processor_vlm.apply_chat_template(\n",
    "    conversation, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ").to(model_vlm.device)\n",
    "\n",
    "outputs = model_vlm.generate(**inputs, max_new_tokens=64)\n",
    "decoded = processor_vlm.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "reading_text = decoded.split()[-2]  # e.g. \"7.36\"\n",
    "meter_reading = float(reading_text)\n",
    "\n",
    "print(f\"\\n--- Meter Reading ---\\n{meter_reading} mm\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Define functions\n",
    "# ----------------------------\n",
    "def turn_on_light(room: str):\n",
    "    return {\"status\": f\"Light in {room} turned ON\"}\n",
    "\n",
    "def turn_off_light(room: str):\n",
    "    return {\"status\": f\"Light in {room} turned OFF\"}\n",
    "\n",
    "tools_json = [\n",
    "    {\n",
    "        \"name\": \"turn_on_light\",\n",
    "        \"description\": \"Turn on the light in a specified room\",\n",
    "        \"parameters\": {\"type\": \"object\", \"properties\": {\"room\": {\"type\": \"string\"}}, \"required\": [\"room\"]}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"turn_off_light\",\n",
    "        \"description\": \"Turn off the light in a specified room\",\n",
    "        \"parameters\": {\"type\": \"object\", \"properties\": {\"room\": {\"type\": \"string\"}}, \"required\": [\"room\"]}\n",
    "    }\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Load LLM\n",
    "# ----------------------------\n",
    "model_id_llm = \"LiquidAI/LFM2-350M\"\n",
    "model_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_llm,\n",
    "    device_map={\"\": \"cpu\"},\n",
    "    torch_dtype=\"float32\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer_llm = AutoTokenizer.from_pretrained(model_id_llm)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Build decision prompt\n",
    "# ----------------------------\n",
    "prompt = f\"\"\"\n",
    "<|startoftext|><|im_start|>system\n",
    "List of tools: <|tool_list_start|>{json.dumps(tools_json)}<|tool_list_end|><|im_end|>\n",
    "<|im_start|>user\n",
    "The meter reading is {meter_reading} mm.\n",
    "If the reading is greater than 80 mm, call turn_off_light with room=\"living_room\".\n",
    "If it is 80 or less, do nothing.\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "# Generate LLM decision\n",
    "input_ids = tokenizer_llm.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}],\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model_llm.device)\n",
    "\n",
    "output = model_llm.generate(\n",
    "    input_ids,\n",
    "    temperature=0.2,\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "\n",
    "decoded_llm = tokenizer_llm.decode(output[0], skip_special_tokens=True)\n",
    "print(\"\\n--- LLM Decision ---\")\n",
    "print(decoded_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ca3b9-3dce-4029-82a3-b5a329c9ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 5: Execute function if needed\n",
    "# ----------------------------\n",
    "if \"turn_off_light\" in decoded_llm:\n",
    "    result = turn_off_light(\"living_room\")\n",
    "    print(\"\\n--- Action Taken ---\")\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"\\n--- Action Taken ---\\nNo action (reading safe).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
