# Install required packages first if not installed
# pip install torch transformers

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import json

# Device setup (CPU/GPU/MPS)
device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")

# Load LiquidAI LFM2-350M model and tokenizer
model_name = "LiquidAI/LFM2-350M"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

# --------------------------
# Smart Home Functions
# --------------------------
def turn_on_light(room: str):
    return {"status": f"Light in {room} turned ON"}

def turn_off_light(room: str):
    return {"status": f"Light in {room} turned OFF"}

def set_thermostat(temp: int):
    return {"status": f"Thermostat set to {temp}Â°C"}

def lock_door(door: str):
    return {"status": f"{door} door locked"}

def unlock_door(door: str):
    return {"status": f"{door} door unlocked"}

TOOLS = {
    "turn_on_light": turn_on_light,
    "turn_off_light": turn_off_light,
    "set_thermostat": set_thermostat,
    "lock_door": lock_door,
    "unlock_door": unlock_door
}

# --------------------------
# Tools JSON for LFM2
# --------------------------
tools_json = [
    {
        "name": "turn_on_light",
        "description": "Turn on the light in a specified room",
        "parameters": {
            "type": "object",
            "properties": {"room": {"type": "string", "description": "Room name"}},
            "required": ["room"]
        }
    },
    {
        "name": "turn_off_light",
        "description": "Turn off the light in a specified room",
        "parameters": {
            "type": "object",
            "properties": {"room": {"type": "string", "description": "Room name"}},
            "required": ["room"]
        }
    },
    {
        "name": "set_thermostat",
        "description": "Set the thermostat temperature",
        "parameters": {
            "type": "object",
            "properties": {"temp": {"type": "integer", "description": "Target temperature"}},
            "required": ["temp"]
        }
    },
    {
        "name": "lock_door",
        "description": "Lock a door",
        "parameters": {
            "type": "object",
            "properties": {"door": {"type": "string", "description": "Door name"}},
            "required": ["door"]
        }
    },
    {
        "name": "unlock_door",
        "description": "Unlock a door",
        "parameters": {
            "type": "object",
            "properties": {"door": {"type": "string", "description": "Door name"}},
            "required": ["door"]
        }
    }
]

# --------------------------
# Helper: Parse multiple calls respecting parentheses
# --------------------------
def parse_multiple_func_calls(func_calls_str):
    calls = []
    current = ""
    paren_level = 0
    for char in func_calls_str:
        if char == "(":
            paren_level += 1
        elif char == ")":
            paren_level -= 1
        if char == "," and paren_level == 0:
            calls.append(current.strip())
            current = ""
        else:
            current += char
    if current:
        calls.append(current.strip())
    return calls

# --------------------------
# Smart Home Assistant Loop
# --------------------------
def run_smart_home():
    print("Welcome to Smart Home Assistant! Type 'exit' to quit.\n")
    while True:
        user_input = input("User: ")
        if user_input.lower() == "exit":
            break

        prompt = (
            "<|startoftext|><|im_start|>system\n"
            f"List of tools: <|tool_list_start|>{json.dumps(tools_json)}<|tool_list_end|><|im_end|>\n"
            f"<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n"
        )

        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        outputs = model.generate(**inputs, max_new_tokens=300)
        decoded = tokenizer.decode(outputs[0])

        # Extract all function calls
        if "<|tool_call_start|>" in decoded:
            import re
            pattern = r"<\|tool_call_start\|>(.*?)<\|tool_call_end\|>"
            matches = re.findall(pattern, decoded, re.DOTALL)

            for func_call_str in matches:
                func_call_str = func_call_str.strip()
                # Remove surrounding brackets
                if func_call_str.startswith("[") and func_call_str.endswith("]"):
                    func_call_str = func_call_str[1:-1].strip()

                # Split multiple function calls correctly
                func_calls = parse_multiple_func_calls(func_call_str)

                for call in func_calls:
                    print(f"\n[Function call generated by LFM2]: {call}")
                    try:
                        func_name = call.split("(")[0]
                        args_str = call.split("(")[1].rstrip(")")
                        args_dict = {}
                        if args_str:
                            for pair in parse_multiple_func_calls(args_str):  # reuse parser for args
                                k, v = pair.split("=")
                                v = v.strip()
                                # Convert numeric values
                                if v.isdigit():
                                    v = int(v)
                                else:
                                    v = v.strip('"')
                                args_dict[k.strip()] = v

                        result = TOOLS[func_name](**args_dict)
                        print(f"[Smart Home System Response]: {result['status']}\n")

                    except Exception as e:
                        print(f"[Error executing function]: {e}\n")
        else:
            print(f"[Assistant]: {decoded}\n")


if __name__ == "__main__":
    run_smart_home()
